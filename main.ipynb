{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5914ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [22/May/2024 05:12:24] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:24] \"GET /lib/owlcarousel/assets/owl.carousel.min.css HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:25] \"GET /static/feature.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:26] \"GET /img/overlay-bottom.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:26] \"GET /static/bg.jpg HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:26] \"GET /img/overlay-top.png HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:26] \"GET /img/bg-image.jpg HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:28] \"GET /img/favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [22/May/2024 05:12:30] \"GET /img/favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://maps.google.com/maps%3Fq%3DTo%2Bdeploy%2Bon%2Ba%2Breal%2Bworld%2Bdataset%250ATo%2Bdesign%2Ba%2Bnovel%26um%3D1%26ie%3DUTF-8%26ved%3D1t:200713%26ictx%3D111: 404 Client Error: Not Found for url: https://maps.google.com/maps%3Fq%3DTo%2Bdeploy%2Bon%2Ba%2Breal%2Bworld%2Bdataset%250ATo%2Bdesign%2Ba%2Bnovel%26um%3D1%26ie%3DUTF-8%26ved%3D1t:200713%26ictx%3D111\n",
      "Error fetching https://moez-62905.medium.com/easily-deploy-machine-learning-models-from-the-comfort-of-your-notebook-9068a88f4cf5: 403 Client Error: Forbidden for url: https://moez-62905.medium.com/easily-deploy-machine-learning-models-from-the-comfort-of-your-notebook-9068a88f4cf5\n",
      "Error fetching https://towardsdatascience.com/from-design-to-deploy-the-whole-lifepath-of-a-machine-learning-app-e9e0357525cc: 403 Client Error: Forbidden for url: https://towardsdatascience.com/from-design-to-deploy-the-whole-lifepath-of-a-machine-learning-app-e9e0357525cc\n",
      "Error fetching https://www.springboard.com/blog/data-analytics/data-analysis-projects/: 403 Client Error: Forbidden for url: https://www.springboard.com/blog/data-analytics/data-analysis-projects/\n",
      "Error fetching https://medium.com/%40sourabhpotnis/data-science-model-deployment-9999de74f005: 403 Client Error: Forbidden for url: https://medium.com/%40sourabhpotnis/data-science-model-deployment-9999de74f005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2024 05:15:38] \"POST /check HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253DTo%252Bdeploy%252Bon%252Ba%252Breal%252Bworld%252Bdataset%25250ATo%252Bdesign%252Ba%252Bnove%26hl%3Den: 404 Client Error: Not Found for url: https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253DTo%252Bdeploy%252Bon%252Ba%252Breal%252Bworld%252Bdataset%25250ATo%252Bdesign%252Ba%252Bnove%26hl%3Den\n",
      "Error fetching https://maps.google.com/maps%3Fq%3DTo%2Bgive%2Ba%2Bdeeper%2Bunderstanding%2Bof%2Bthe%2Bfashion%2Breco%26um%3D1%26ie%3DUTF-8%26ved%3D1t:200713%26ictx%3D111: 404 Client Error: Not Found for url: https://maps.google.com/maps%3Fq%3DTo%2Bgive%2Ba%2Bdeeper%2Bunderstanding%2Bof%2Bthe%2Bfashion%2Breco%26um%3D1%26ie%3DUTF-8%26ved%3D1t:200713%26ictx%3D111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/May/2024 05:20:34] \"POST /check HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253DTo%252Bgive%252Ba%252Bdeeper%252Bunderstanding%252Bof%252Bthe%252Bfashion%252Breco%26hl%3Den: 404 Client Error: Not Found for url: https://accounts.google.com/ServiceLogin%3Fcontinue%3Dhttps://www.google.com/search%253Fq%253DTo%252Bgive%252Ba%252Bdeeper%252Bunderstanding%252Bof%252Bthe%252Bfashion%252Breco%26hl%3Den\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric, strip_short\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/check', methods=['POST'])\n",
    "def check_plagiarism():\n",
    "    user_input = request.json.get('text')\n",
    "    if not user_input:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400\n",
    "    \n",
    "    try:\n",
    "        result = check_text_for_plagiarism(user_input)\n",
    "        return jsonify(result)\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "def scrape_web(query):\n",
    "    search_url = f\"https://www.google.com/search?q={query}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\"\n",
    "    }\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    results = []\n",
    "    for g in soup.find_all('a'):\n",
    "        link = g.get('href')\n",
    "        if link and 'url?q=' in link:\n",
    "            results.append(link.split('url?q=')[1].split('&')[0])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def fetch_page_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        page_content = ' '.join([para.text for para in paragraphs])\n",
    "        return page_content\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess(text):\n",
    "    custom_filters = [strip_punctuation, strip_numeric, strip_short]\n",
    "    return preprocess_string(text, custom_filters)\n",
    "\n",
    "def compare_texts(text1, text2):\n",
    "    texts = [preprocess(text1), preprocess(text2)]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus])\n",
    "    \n",
    "    vec_bow = dictionary.doc2bow(preprocess(text2))\n",
    "    vec_lsi = lsi[vec_bow]\n",
    "    \n",
    "    sims = index[vec_lsi]\n",
    "    return float(sims[0])\n",
    "\n",
    "def check_text_for_plagiarism(text):\n",
    "    search_results = scrape_web(text[:50])\n",
    "    results = []\n",
    "    for url in search_results:\n",
    "        page_content = fetch_page_content(url)\n",
    "        if page_content:\n",
    "            similarity_score = compare_texts(text, page_content)\n",
    "            if similarity_score > 0:\n",
    "                results.append({\n",
    "                    \"url\": url,\n",
    "                    \"similarity\": similarity_score,\n",
    "                    \"content\": page_content[:500]  # Limiting to the first 500 characters for brevity\n",
    "                })\n",
    "    \n",
    "    if results:\n",
    "        best_match = max(results, key=lambda x: x['similarity'])\n",
    "        best_match['similarity'] = round(best_match['similarity'], 4)\n",
    "    else:\n",
    "        best_match = {\"url\": \"\", \"similarity\": 0.0, \"content\": \"\"}\n",
    "\n",
    "    return best_match\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4310c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16c25fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
